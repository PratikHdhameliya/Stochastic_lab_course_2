n_runs <- 100   # Number of samples
N_samples <- 2000  # size of samples
gamma_value <- 0.85  # Gamma parameter value
sample_results <- generate_from_F(n_runs, N_samples, gamma_value)
df_samples <- data.frame(sample_results)
# Initialize the list to store the results
hill_estimator_F <- vector("list", 100)
# Loop over the columns of the data frame
for (i in seq_len(ncol(df_samples))) {
hill_estimator_F[i] <- Hill_estimator(df_samples[, i], 200)
}
print(hill_estimator_F)
#(2)#####################
p_th_quantile <- function(data, k, gamma, p) {
# Ensure data is numeric
data <- as.numeric(data)
# Get the number of data points
n <- length(data)
# Sort data in decreasing order
sorted_data <- sort(data, decreasing = TRUE)
# Identify the k-th last data point
last_k <- sorted_data[k]
# Calculate the scale parameter a_n
a_n <- last_k * gamma
# Calculate multiplier for adjustment
if (n * p < 0) {
stop("p times number of data points must be at least 1")
}
multiplier <- ((k / (n * p))^gamma - 1) / gamma
# Return the estimated p-th quantile
return(last_k + (a_n * multiplier))
}
calculate_quantiles_for_df <- function(df, k, gamma_list, p) {
# Ensure gamma_list length matches the number of columns in df
if (length(gamma_list) != ncol(df)) {
stop("Length of gamma_list must be equal to the number of columns in the dataframe.")
}
# Initialize a list or vector to store the quantile results for each column
quantile_results <- list(ncol(df))
# Iterate through each column of the data frame
for (i in seq_along(df)) {
# Apply the quantile function to each column using the corresponding gamma value
quantile_results[[i]] <- p_th_quantile(df[,i], k, gamma_list[[i]], p)
names(quantile_results)[i] <- names(df)[i]  # Label the results with column names
}
# Return the list of quantiles
return(quantile_results)
}
# Parameters
k <- 200
gamma_list <- hill_estimator_F  # Specific gamma for each column of dataset
p <- 0.0001
# Calculate the quantiles
quantile_results <- calculate_quantiles_for_df(df_samples, k, gamma_list, p)
df <- as.data.frame(t(unlist(quantile_results)))
print(df)
quantile_results <- list(100)
#(2)#####################
p_th_quantile <- function(data, k, gamma, p) {
# Ensure data is numeric
data <- as.numeric(data)
# Get the number of data points
n <- length(data)
# Sort data in decreasing order
sorted_data <- sort(data, decreasing = TRUE)
# Identify the k-th last data point
last_k <- sorted_data[k]
# Calculate the scale parameter a_n
a_n <- last_k * gamma
# Calculate multiplier for adjustment
if (n * p < 0) {
stop("p times number of data points must be at least 1")
}
multiplier <- ((k / (n * p))^gamma - 1) / gamma
# Return the estimated p-th quantile
return(last_k + (a_n * multiplier))
}
calculate_quantiles_for_df <- function(df, k, gamma_list, p) {
# Ensure gamma_list length matches the number of columns in df
if (length(gamma_list) != ncol(df)) {
stop("Length of gamma_list must be equal to the number of columns in the dataframe.")
}
# Initialize a list or vector to store the quantile results for each column
quantile_results <- list(100)
# Iterate through each column of the data frame
for (i in seq_along(df)) {
# Apply the quantile function to each column using the corresponding gamma value
quantile_results[[i]] <- p_th_quantile(df[,i], k, gamma_list[[i]], p)
names(quantile_results)[i] <- names(df)[i]  # Label the results with column names
}
# Return the list of quantiles
return(quantile_results)
}
# Parameters
k <- 200
gamma_list <- hill_estimator_F  # Specific gamma for each column of dataset
p <- 0.0001
# Calculate the quantiles
quantile_results <- calculate_quantiles_for_df(df_samples, k, gamma_list, p)
df <- as.data.frame(t(unlist(quantile_results)))
print(df)
p_th_quantile <- function(data, k, gamma, p) {
# Ensure data is numeric
data <- as.numeric(data)
# Get the number of data points
n <- length(data)
# Sort data in decreasing order
sorted_data <- sort(data, decreasing = TRUE)
# Identify the k-th last data point
last_k <- sorted_data[k]
# Calculate the scale parameter a_n
a_n <- last_k * gamma
# Calculate multiplier for adjustment
if (n * p < 0) {
stop("p times number of data points must be at least 1")
}
multiplier <- ((k / (n * p))^gamma - 1) / gamma
# Return the estimated p-th quantile
return(last_k + (a_n * multiplier))
}
calculate_quantiles_for_df <- function(df, k, gamma_list, p) {
# Ensure gamma_list length matches the number of columns in df
if (length(gamma_list) != ncol(df)) {
stop("Length of gamma_list must be equal to the number of columns in the dataframe.")
}
# Initialize a list or vector to store the quantile results for each column
quantile_results <- list(100)
# Iterate through each column of the data frame
for (i in seq_along(df)) {
# Apply the quantile function to each column using the corresponding gamma value
quantile_results[[i]] <- p_th_quantile(df[,i], k, gamma_list[[i]], p)
#names(quantile_results)[i] <- names(df)[i]  # Label the results with column names
}
# Return the list of quantiles
return(quantile_results)
}
# Parameters
k <- 200
gamma_list <- hill_estimator_F  # Specific gamma for each column of dataset
p <- 0.0001
# Calculate the quantiles
quantile_results <- calculate_quantiles_for_df(df_samples, k, gamma_list, p)
df <- as.data.frame(t(unlist(quantile_results)))
print(df)
p_th_quantile <- function(data, k, gamma, p) {
# Ensure data is numeric
data <- as.numeric(data)
# Get the number of data points
n <- length(data)
# Sort data in decreasing order
sorted_data <- sort(data, decreasing = TRUE)
# Identify the k-th last data point
last_k <- sorted_data[k]
# Calculate the scale parameter a_n
a_n <- last_k * gamma
# Calculate multiplier for adjustment
if (n * p < 0) {
stop("p times number of data points must be at least 1")
}
multiplier <- ((k / (n * p))^gamma - 1) / gamma
# Return the estimated p-th quantile
return(last_k + (a_n * multiplier))
}
calculate_quantiles_for_df <- function(df, k, gamma_list, p) {
# Ensure gamma_list length matches the number of columns in df
if (length(gamma_list) != ncol(df)) {
stop("Length of gamma_list must be equal to the number of columns in the dataframe.")
}
# Initialize a list or vector to store the quantile results for each column
quantile_results <- vector("list", 100)
# Iterate through each column of the data frame
for (i in seq_along(df)) {
# Apply the quantile function to each column using the corresponding gamma value
quantile_results[[i]] <- p_th_quantile(df[,i], k, gamma_list[[i]], p)
#names(quantile_results)[i] <- names(df)[i]  # Label the results with column names
}
# Return the list of quantiles
return(quantile_results)
}
# Parameters
k <- 200
gamma_list <- hill_estimator_F  # Specific gamma for each column of dataset
p <- 0.0001
# Calculate the quantiles
quantile_results <- calculate_quantiles_for_df(df_samples, k, gamma_list, p)
df <- as.data.frame(t(unlist(quantile_results)))
print(df)
p_th_quantile <- function(data, k, gamma, p) {
# Ensure data is numeric
data <- as.numeric(data)
# Get the number of data points
n <- length(data)
# Sort data in decreasing order
sorted_data <- sort(data, decreasing = TRUE)
# Identify the k-th last data point
last_k <- sorted_data[k]
# Calculate the scale parameter a_n
a_n <- last_k * gamma
# Calculate multiplier for adjustment
if (n * p < 0) {
stop("p times number of data points must be at least 1")
}
multiplier <- ((k / (n * p))^gamma - 1) / gamma
# Return the estimated p-th quantile
return(last_k + (a_n * multiplier))
}
calculate_quantiles_for_df <- function(df, k, gamma_list, p) {
# Ensure gamma_list length matches the number of columns in df
if (length(gamma_list) != ncol(df)) {
stop("Length of gamma_list must be equal to the number of columns in the dataframe.")
}
# Initialize a list or vector to store the quantile results for each column
quantile_results <- vector("list", 100)
# Iterate through each column of the data frame
for (i in seq_along(df)) {
# Apply the quantile function to each column using the corresponding gamma value
quantile_results[[i]] <- p_th_quantile(df[,i], k, gamma_list[[i]], p)
# Assign names as string indices from 1 to 100
names(quantile_results) <- as.character(1:length(quantile_results))
}
# Return the list of quantiles
return(quantile_results)
}
# Parameters
k <- 200
gamma_list <- hill_estimator_F  # Specific gamma for each column of dataset
p <- 0.0001
# Calculate the quantiles
quantile_results <- calculate_quantiles_for_df(df_samples, k, gamma_list, p)
df <- as.data.frame(t(unlist(quantile_results)))
print(df)
p_cap <- function(data, c, k, gamma) {
# Ensure data is numeric
data <- as.numeric(data)
# Validate input
if(k <= 0 || k > length(data)) {
stop("Parameter k must be within the range of 1 to the number of elements in data.")
}
if(gamma == 0) {
stop("Gamma should not be zero.")
}
# Get the number of data points
n <- length(data)
# Sort data in decreasing order
sorted_data <- sort(data, decreasing = TRUE)
# Identify the k-th last data point
last_k <- sorted_data[k]
# Calculate the scale parameter a_n
a_n <- last_k * gamma
# Compute the multiplier factor considering threshold c
multiplier <- (max(0, 1 + gamma * ((c - last_k) / a_n)))^(-1 / gamma)
# Calculate the adjusted probability
return((k / n) * multiplier)
}
a<-p_cap(df_samples[,1],4.171,200,0.3232)
calculate_P_cap <- function(df, k, gamma_list, c) {
# Ensure gamma_list length matches the number of columns in df
if (length(gamma_list) != ncol(df)) {
stop("Length of gamma_list must be equal to the number of columns in the dataframe.")
}
# Initialize a list or vector to store the quantile results for each column
p_cap_list <- list(ncol(df))
# Iterate through each column of the data frame
for (i in seq_along(df)) {
# Apply the quantile function to each column using the corresponding gamma value
quantile_results[[i]] <- p_cap(df[,i], c, k,gamma_list[[i]])
# Assign names as string indices from 1 to 100
names(quantile_results) <- as.character(1:length(quantile_results))
}
# Return the list of quantiles
return(quantile_results)
}
p_caps<-calculate_P_cap(df_samples,200,hill_estimator_F,4.171)
View(p_caps)
#to see the default random generator
print(RNGkind())
RNGkind(kind="Wichmann-Hill")
#Now random generator is Wichmann-Hill
print(RNGkind())
set.seed(123)
# Parameters
N <- 1000
p <- 0.4
#########################################################################################
# Function to generate geometrically distributed random variables using inversion method
generate_geometric <- function(N, p) {
# Generate uniform random variables
u <- runif(N)
# Inversion method
return(ceiling(log(1 - u) / log(1 - p)))
}
# Generate geometrically distributed random variables
geo_samples_Inverse <- generate_geometric(N, p)
########################################################################################
# Simulate Bernoulli trials
bernoulli_trials <- rbinom(N, size = 1, prob = p)
# Initialize vector to store geometrically distributed random variables
geo_samples_Bernouli <- integer(N)
# Simulate geometrically distributed random variables
for (i in 1:N) {
# Count the number of failures until the first success
num_failures <- 0
while (bernoulli_trials[i] == 0) {
num_failures <- num_failures + 1
bernoulli_trials[i] <- rbinom(1, size = 1, prob = p)
}
# Store the number of failures as the geometric variable
geo_samples_Bernouli[i] <- num_failures
}
###############################################################################################
# Simulate geometrically distributed random variables
geo_samples_Rgeom <- rgeom(N, prob = p)
##############################################################################################
# Display the simulated geometrically distributed random variables
print(geo_samples_Bernouli)
print(geo_samples_Inverse)
print(geo_samples_Rgeom)
# Plot histograms for all three datasets in one plot
hist(geo_samples_Bernouli, col = "skyblue", main = "Empirical proabability density functions", xlab = "Value", ylab = "Frequency", xlim = c(0, max(c(geo_samples_Bernouli, geo_samples_Inverse, geo_samples_Rgeom))))
# Add histogram for geo_samples_Inverse
hist(geo_samples_Inverse, col = "lightgreen", add = TRUE)
# Add histogram for geo_samples_Rgeom
hist(geo_samples_Rgeom, col = "lightcoral", add = TRUE)
# Add legend
legend("topright", legend = c("Bernoulli Inversion", "Inverse Method", "rgeom Function"), fill = c("skyblue", "lightgreen", "lightcoral"))
RNGkind(kind = "Mersenne-Twister")
RNGkind(kind = "Mersenne-Twister")
print(RNGkind())
# Plot histograms for all three datasets in one plot with lines
hist(geo_samples_Bernouli, col = "white", main = "Empirical Probability Density Functions", xlab = "Value", ylab = "Density", xlim = c(0, max(c(geo_samples_Bernouli, geo_samples_Inverse, geo_samples_Rgeom))), freq = FALSE)
lines(density(geo_samples_Bernouli), col = "skyblue", lwd = 2)
# Add density plot for geo_samples_Inverse
lines(density(geo_samples_Inverse), col = "lightgreen", lwd = 2)
# Add density plot for geo_samples_Rgeom
lines(density(geo_samples_Rgeom), col = "lightcoral", lwd = 2)
# Add legend
legend("topright", legend = c("Bernoulli Inversion", "Inverse Method", "rgeom Function"), fill = c("skyblue", "lightgreen", "lightcoral"), lwd = 2)
# Plot density lines for all three datasets in one plot
plot(density(geo_samples_Bernouli), col = "skyblue", lwd = 2, main = "Empirical Probability Density Functions", xlab = "Value", ylab = "Density", xlim = c(0, max(c(geo_samples_Bernouli, geo_samples_Inverse, geo_samples_Rgeom))))
lines(density(geo_samples_Inverse), col = "lightgreen", lwd = 2)
lines(density(geo_samples_Rgeom), col = "lightcoral", lwd = 2)
# Add legend
legend("topright", legend = c("Bernoulli Inversion", "Inverse Method with geometric", "rgeom Function"), fill = c("skyblue", "lightgreen", "lightcoral"), lwd = 2)
set.seed(123)
# Parameters
N <- 1000
# Define the standard normal density function f(x)
f <- function(x) {
exp(-x^2 / 2) / sqrt(2 * pi)
}
# Define the standard Laplace density function g(x)
g <- function(x) {
exp(-abs(x)) / 2
}
# Define the range of x values
x <- runif(N)
# Find the maximum ratio
c_value <- max(f(x) / g(x))
cat("The value of c such that f(x) <= c * g(x) is:", c_value, "\n")
##########2nd_PART_OF_B###########
generate_laplace <- function(N) {
u <- runif(N)
laplace_samples <- -sign(u - 0.5) * log(1 - 2 * abs(u - 0.5))
return(laplace_samples)
}
generate_normal_accept_reject <- function(N) {
normal_samples <- numeric(N)
count_accept <- 0
count_reject <- 0  # Initialize rejection counter
for (i in 1:N) {
while (TRUE) {
x <- generate_laplace(1)
u <- runif(1)
if (u <  f(x)/ (c_value * g(x))) {
normal_samples[i] <- x
count_accept <- count_accept + 1
} else {
count_reject <- count_reject + 1  # Increment rejection counter
}
}
}
acceptance_probability <- count_accept / N
rejection_probability <- count_reject / N  # Calculate rejection probability
return(list(samples = normal_samples, acceptance_probability = acceptance_probability,
rejection_probability = rejection_probability))  # Include rejection probability in return value
}
result <- generate_normal_accept_reject(N)
set.seed(123)  # Set the seed for reproducibility
# Parameters
N <- 1000  # Number of samples to generate
# Define the standard normal density function f(x)
f <- function(x) {
exp(-x^2 / 2) / sqrt(2 * pi)
}
# Define the standard Laplace density function g(x)
g <- function(x) {
exp(-abs(x)) / 2
}
# Define the range of x values
x <- runif(N)
# Find the maximum ratio
c_value <- max(f(x) / g(x))  # Calculate the maximum value of f(x)/g(x) as c
cat("The value of c such that f(x) <= c * g(x) is:", c_value, "\n")
# Function to generate Laplace distributed random variables
generate_laplace <- function(N) {
u <- runif(N)  # Generate uniform random variables
laplace_samples <- -sign(u - 0.5) * log(1 - 2 * abs(u - 0.5))  # Inverse transform sampling for Laplace distribution
return(laplace_samples)
}
# Function to generate standard normal distributed random variables using the accept-reject method
generate_normal_accept_reject <- function(N) {
normal_samples <- numeric(N)  # Initialize vector to store normal samples
count_accept <- 0  # Initialize acceptance counter
count_reject <- 0  # Initialize rejection counter
for (i in 1:N) {
while (TRUE) {
x <- generate_laplace(1)  # Generate a Laplace sample
u <- runif(1)  # Generate a uniform random variable
if (u <  f(x) / (c_value * g(x))) {  # Check the accept-reject condition
normal_samples[i] <- x  # Accept the sample
count_accept <- count_accept + 1  # Increment acceptance counter
break  # Exit the while loop
} else {
count_reject <- count_reject + 1  # Increment rejection counter
}
}
}
acceptance_probability <- count_accept / N  # Calculate acceptance probability
rejection_probability <- count_reject / N  # Calculate rejection probability
return(list(samples = normal_samples, acceptance_probability = acceptance_probability,
rejection_probability = rejection_probability))  # Return the results
}
# Generate normal samples using the accept-reject method
result <- generate_normal_accept_reject(N)
View(result)
normal_samples <- result$samples
acceptance_probability <- result$acceptance_probability
# Compare estimated and theoretical acceptance probabilities
theoretical_acceptance_probability <- c_value
cat("Theoretical acceptance probability:", theoretical_acceptance_probability, "\n")
cat("Estimated acceptance probability:", acceptance_probability, "\n")
# Plot histogram of obtained sample with standard normal density
hist(normal_samples, breaks = 30, freq = FALSE, main = "Histogram of Sample", xlab = "Value", ylab = "Density",xlim=c(-4,4))  # Plot histogram
curve(f(x), from = -4, to = 4, col = "blue", lwd = 2, add = TRUE, n = 1000, yaxt = "n")  # Plot standard normal density
# Make QQ-plot
qqnorm(normal_samples)  # Plot QQ-plot
qqline(normal_samples, col = "red")  # Add reference line
set.seed(123)  # Set the seed for reproducibility
# Parameters
N <- 1000  # Number of samples to generate
# Define the standard normal density function f(x)
f <- function(x) {
exp(-x^2 / 2) / sqrt(2 * pi)
}
# Define the standard Laplace density function g(x)
g <- function(x) {
exp(-abs(x)) / 2
}
# Define the range of x values
x <- runif(N)
# Find the maximum ratio
c_value <- max(f(x) / g(x))  # Calculate the maximum value of f(x)/g(x) as c
cat("The value of c such that f(x) <= c * g(x) is:", c_value, "\n")
###############################################################################################
# Function to generate Laplace distributed random variables
generate_laplace <- function(N) {
u <- runif(N)  # Generate uniform random variables
laplace_samples <- -sign(u - 0.5) * log(1 - 2 * abs(u - 0.5))  # Inverse transform sampling for Laplace distribution
return(laplace_samples)
}
# Function to generate standard normal distributed random variables using the accept-reject method
generate_normal_accept_reject <- function(N) {
normal_samples <- numeric(N)  # Initialize vector to store normal samples
count_accept <- 0  # Initialize acceptance counter
count_reject <- 0  # Initialize rejection counter
for (i in 1:N) {
while (TRUE) {
x <- generate_laplace(1)  # Generate a Laplace sample
u <- runif(1)  # Generate a uniform random variable
if (u <  f(x) / (c_value * g(x))) {  # Check the accept-reject condition
normal_samples[i] <- x  # Accept the sample
count_accept <- count_accept + 1  # Increment acceptance counter
break  # Exit the while loop
} else {
count_reject <- count_reject + 1  # Increment rejection counter
}
}
}
acceptance_probability <- count_accept / N  # Calculate acceptance probability
rejection_probability <- count_reject / N  # Calculate rejection probability
return(list(samples = normal_samples, acceptance_probability = acceptance_probability,
rejection_probability = rejection_probability))  # Return the results
}
# Generate normal samples using the accept-reject method
result <- generate_normal_accept_reject(N)
normal_samples <- result$samples
acceptance_probability <- result$acceptance_probability
# Compare estimated and theoretical acceptance probabilities
theoretical_acceptance_probability <- c_value
# Create a data frame for the output
output_df <- data.frame(
Metric = c("Theoretical acceptance probability", "Estimated acceptance probability"),
Value = c(theoretical_acceptance_probability, acceptance_probability)
)
# Export the data frame to a CSV file
write.csv(output_df, "Comparision of both probability.csv", row.names = FALSE)
# Plot histogram of obtained sample with standard normal density
hist(normal_samples, breaks = 30, freq = FALSE, main = "Histogram of Standard Normal Samples with Density of Standard Normal ", xlab = "Value", ylab = "Frequency",xlim=c(-4,4))  # Plot histogram
curve(f(x), from = -4, to = 4, col = "blue", lwd = 2, add = TRUE, n = 1000, yaxt = "n")  # Plot standard normal density
# Make QQ-plot
qqnorm(normal_samples)  # Plot QQ-plot
qqline(normal_samples, col = "red")  # Add reference line
