# Load necessary libraries
library(glmnet)
# Set seed for reproducibility
set.seed(123)
# Generate data
n <- 100  # Number of samples
p <- 3    # Number of predictors
# Create design matrix X
X <- cbind(rep(1, n),            # Intercept term
seq(1, n),             # Linear trend
rep(c(1,0), times = n/2))  # Binary predictor with alternating values
# True coefficients for generating the response variable
beta_true <- c(3, 0, 1/n)
# Generate response variable y with noise
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
# Perform Lasso regression with cross-validation
lasso_model_cv <- cv.glmnet(X, y, alpha = 1, family = "gaussian")
# Plot the cross-validation results
plot(lasso_model_cv)
# Find lambda value for minimum cross-validation error
lambda_min_lasso <- lasso_model_cv$lambda.min
cat("Lambda for minimum CV error:", lambda_min_lasso, "\n")
# Fit Lasso model with selected lambda
lasso_fit <- glmnet(X, y, alpha = 1, lambda = lambda_min_lasso)
lasso_coef <- coef(lasso_fit)
print(lasso_coef)
# Normal OLS beta_hat for adaptive lasso
ols_fit <- lm(y ~ X + 0)  # Fit OLS model without intercept
ols_coef <- coef(ols_fit)  # Extract OLS coefficients
# Perform Adaptive Lasso regression with cross-validation
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE)
# Plot the cross-validation results for adaptive Lasso
plot(adapt_model_cv)
# Find lambda value for minimum cross-validation error
lambda_min_adaptive <- adapt_model_cv$lambda.min
cat("Lambda for minimum CV error:", lambda_min_adaptive, "\n")
# Fit Adaptive Lasso model with selected lambda
adapt_lasso_fit <- glmnet(X, y, alpha = 1, lambda = lambda_min_adaptive, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE)
adapt_lasso_coef <- coef(adapt_lasso_fit)
print(adapt_lasso_coef)
plot(lasso_model_cv, main = "Cross-Validation Plot for Lasso Regression")
View(lasso_model_cv)
plot(lasso_model_cv, ylab = "Mean-Squared Error for Lasso Regression")
plot(results_df$lambda, results_df$CV, type = "l", xlab = "\Lambda", ylab = "CV")
# Plot the cross-validation results for adaptive Lasso
plot(adapt_model_cv,ylab = "Mean-Squared Error for adaptive Lasso Regression")
# Load the glmnet package
library(glmnet)
#this function runs 1000 times and count error and lambda and give mean value
# Extract specific information from results, e.g., BIC_error
#BIC_errors <- sapply(results, function(x) x$BIC_error)
run_adaptive_lasso <- function(beta_true) {
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- list(
lambda_min = mean(lambda_min_adoptive_values),
cv_error = mean(cv_error_values),
BIC_error = mean(BIC_error_values),
prediction_error = mean(prediction_error)
)
return(results)
}
beta_true <- c(3, 0, 1/n)
result<-run_adaptive_lasso(beta_true)
print(result)
beta_trues <- list(
c(1, 2, 3),
c(1, 2.5, 3),
c(1, 3, 3),
c(1, 3.5, 3),
c(1, 4, 3)
)
# Apply the function to each beta_true vector
results <- lapply(beta_trues, run_adaptive_lasso)
print(results)
#BIC_errors <- sapply(results, function(x) x$BIC_error)
run_adaptive_lasso <- function(beta_true) {
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
results_mean <- list(
lambda_min = mean(lambda_min_adoptive_values),
cv_error = mean(cv_error_values),
BIC_error = mean(BIC_error_values),
prediction_error = mean(prediction_error)
)
return(results,results_mean)
}
beta_true <- c(3, 0, 1/n)
result<-run_adaptive_lasso(beta_true)
#BIC_errors <- sapply(results, function(x) x$BIC_error)
run_adaptive_lasso <- function(beta_true) {
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
beta_true <- c(3, 0, 1/n)
result<-run_adaptive_lasso(beta_true)
View(result)
ggplot(result, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
ggplot(result, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
ggplot(result, aes(x = lambda_min, y = prediction_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
ggplot(result, aes(x = lambda_min, y = prediction_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
ggplot(result, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
cv_error
ggplot(result, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
print(mean(result$lambda_min))
print(mean(result$cv_error))
print(mean(result$BIC_error))
print(mean(result$prediction_error))
lamda_av<-mean(result$lambda_min)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
adapt_model_cv <- bic.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
BIC_error <- function(lambda) {
beta_true <- c(3, 0, 1/n)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values<- RSS + log(n) * num_nonzero
return(BIC_error_values)
}
error<-BIC_error(52.309)
BIC_error <- function(lambda) {
beta_true <- c(3, 0, 1/n)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values<- RSS + log(n) * num_nonzero
return(BIC_error_values)
}
error<-BIC_error(52.309)
install.packages("DEoptim")
library(DEoptim)
# Set the algorithm parameters for DEoptim
control <- DEoptim.control(itermax = 100, F = 0.5, CR = 0.9)
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 10, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
# Set the algorithm parameters for DEoptim
control <- DEoptim.control(itermax = 100, F = 0.05, CR = 0.09)
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
# Set the algorithm parameters for DEoptim
control <- DEoptim.control(itermax = 100, F = 0.05, CR = 0.09)
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
# Initialize storage vectors
lambda_min_values <- numeric(1000)
BIC_error_values <- numeric(1000)
# Loop over 1000 iterations
for (i in 1:1000) {
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min_values[i] <- result$optim$bestmem
# Calculate the BIC error for the lambda_min
BIC_error_values[i] <- result$optim$bestval
}
error<-Prediction_error(52.30)
Prediction_error<-function(lambda){
beta_true <- c(3, 0, 1/n)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
prediction_error <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
return(prediction_error)
}
error<-Prediction_error(52.30)
error<-Prediction_error(52.30)
error_1<-BIC_error(52.30)
error<-Prediction_error(52.30)
error_1<-BIC_error(52.30)
View(results)
View(results)
View(result)
View(results_df)
run_adaptive_lasso <- function(beta_true) {
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
beta_true <- c(3, 0, 1/n)
result<-run_adaptive_lasso(beta_true)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
seed(123)
seed(123)
random.seed(123)
set.seed(123)
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
#this function runs 1000 times and count error and lambda and give mean value
run_adaptive_lasso_CV <- function(beta_true) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
beta_true <- c(3, 0, 1/n)
result_CV<-run_adaptive_lasso_CV(beta_true)
View(result_CV)
ggplot(result_CV, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
BIC_error <- function(lambda) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values<- RSS + log(n) * num_nonzero
return(BIC_error_values)
}
# Initialize storage vectors
lambda_min_values <- numeric(1000)
BIC_error_values <- numeric(1000)
# Loop over 1000 iterations
for (i in 1:1000) {
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min_values[i] <- result$optim$bestmem
# Calculate the BIC error for the lambda_min
BIC_error_values[i] <- result$optim$bestval
}
