}
error<-BIC_error(52.309)
BIC_error <- function(lambda) {
beta_true <- c(3, 0, 1/n)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values<- RSS + log(n) * num_nonzero
return(BIC_error_values)
}
error<-BIC_error(52.309)
install.packages("DEoptim")
library(DEoptim)
# Set the algorithm parameters for DEoptim
control <- DEoptim.control(itermax = 100, F = 0.5, CR = 0.9)
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 10, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
# Set the algorithm parameters for DEoptim
control <- DEoptim.control(itermax = 100, F = 0.05, CR = 0.09)
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
# Set the algorithm parameters for DEoptim
control <- DEoptim.control(itermax = 100, F = 0.05, CR = 0.09)
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min <- result$optim$bestmem
# Print the lambda value
cat("Lambda for minimum BIC error:", lambda_min, "\n")
# Initialize storage vectors
lambda_min_values <- numeric(1000)
BIC_error_values <- numeric(1000)
# Loop over 1000 iterations
for (i in 1:1000) {
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min_values[i] <- result$optim$bestmem
# Calculate the BIC error for the lambda_min
BIC_error_values[i] <- result$optim$bestval
}
error<-Prediction_error(52.30)
Prediction_error<-function(lambda){
beta_true <- c(3, 0, 1/n)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
prediction_error <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
return(prediction_error)
}
error<-Prediction_error(52.30)
error<-Prediction_error(52.30)
error_1<-BIC_error(52.30)
error<-Prediction_error(52.30)
error_1<-BIC_error(52.30)
View(results)
View(results)
View(result)
View(results_df)
run_adaptive_lasso <- function(beta_true) {
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
beta_true <- c(3, 0, 1/n)
result<-run_adaptive_lasso(beta_true)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
seed(123)
seed(123)
random.seed(123)
set.seed(123)
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
y <- X %*% beta_true + epsilon
#this function runs 1000 times and count error and lambda and give mean value
run_adaptive_lasso_CV <- function(beta_true) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
beta_true <- c(3, 0, 1/n)
result_CV<-run_adaptive_lasso_CV(beta_true)
View(result_CV)
ggplot(result_CV, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
BIC_error <- function(lambda) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values<- RSS + log(n) * num_nonzero
return(BIC_error_values)
}
# Initialize storage vectors
lambda_min_values <- numeric(1000)
BIC_error_values <- numeric(1000)
# Loop over 1000 iterations
for (i in 1:1000) {
# Run DEoptim to minimize the BIC_error function
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control)
# Extract the lambda value that minimizes the BIC error
lambda_min_values[i] <- result$optim$bestmem
# Calculate the BIC error for the lambda_min
BIC_error_values[i] <- result$optim$bestval
}
# Read the data
data <- read.csv("StudentsPerformance.csv")
# Extract math scores
math_scores <- data$math.score
# Example usage:
bandwidth_seq <- c(0.09,0.5, 4.26, 6.8)  # Sequence of bandwidth values
plot_kde_for_bandwidth_seq(math_scores, bandwidth_seq, epanechnikov_kernel)
# Define Epanechnikov kernel function
epanechnikov_kernel <- function(u) {
return(0.75 * (1 - u^2) * (abs(u) <= 1))
}
# Define Tridiagonal kernel function
tridiagonal_kernel <- function(u) {
return((1/6) * (2 - abs(u)) * (abs(u) <= 2))
}
# Define Uniform kernel function
uniform_kernel <- function(u) {
return(0.5 * (abs(u) <= 1))
}
# Define Gaussian kernel function
gaussian_kernel <- function(u) {
return(dnorm(u))
}
# Kernel Density Estimation (KDE) function
kde <- function(x, data, bandwidth, kernel) {
# Number of data points
n <- length(data)
# Number of points to estimate
k <- length(x)
# Initialize result vector
result <- numeric(k)
# Loop over each point to estimate
for (i in 1:k) {
# Calculate kernel density estimate for each point
result[i] <- sum(kernel((x[i] - data) / bandwidth)) / (n * bandwidth)
}
return(result)
}
# Function to plot kernel density estimation for a sequence of bandwidth values
plot_kde_for_bandwidth_seq <- function(data, bandwidth_seq, kernel) {
library(ggplot2)
# Define the range of values for evaluation
x <- seq(min(data), max(data), length.out = 100)
# Calculate density estimation for each bandwidth
density_estimation <- lapply(bandwidth_seq, function(bw) kde(x, data, bw, kernel))
# Create a data frame for plotting
plot_data <- data.frame(x = rep(x, length(bandwidth_seq)),
density = unlist(density_estimation),
bandwidth = rep(bandwidth_seq, each = length(x)))
# Plot using ggplot2
ggplot(plot_data, aes(x = x, y = density, color = as.factor(bandwidth))) +
geom_line() +
scale_color_manual(values = rainbow(length(bandwidth_seq))) +
labs(x = "X", y = "Density", color = "Bandwidth") +
ggtitle("Kernel Density Estimation")
}
# Function to plot kernel density estimation for different kernels
plot_kde_for_kernel <- function(data, bandwidth, kernels) {
library(ggplot2)
# Define the range of values for evaluation
x <- seq(min(data), max(data), length.out = 100)
# Calculate density estimation for each kernel
density_estimation <- lapply(kernels, function(kernel) kde(x, data, bandwidth, kernel))
# Create a data frame for plotting
plot_data <- data.frame(x = rep(x, length(kernels)),
density = unlist(density_estimation),
kernel = rep(names(kernels), each = length(x)))
# Plot using ggplot2
ggplot(plot_data, aes(x = x, y = density, color = kernel)) +
geom_line() +
scale_color_manual(values = rainbow(length(kernels))) +
labs(x = "X", y = "Density", color = "Kernel") +
ggtitle("Kernel Density Estimation")
}
# Function to compute the cross-validation score for a given bandwidth
CV <- function(h, data, kernel) {
n <- length(data)
cv_score <- 0
# Compute the range of x
range_x <- range(data)
# Compute the integral of f_hat(x;h)^2 with respect to x
integral <- integrate(function(x) kde(x, data, h, kernel)^2, range_x[1], range_x[2], subdivisions = 10000)$value
cv_score <- cv_score + integral
for (i in 1:n) {
xi <- data[i]
for (j in 1:n) {
xj <- data[j]
if (i != j) {
K <- kernel((xj - xi) / h)
cv_score <- cv_score - (2 / (n * (n - 1) * h)) * K
}
}
}
return(cv_score)
}
# Function to compute the optimal bandwidth using cross-validation
compute_optimal_bandwidth <- function(data, bandwidth_values, kernel) {
CV_scores <- numeric(length(bandwidth_values))
# Compute the cross-validation score for each bandwidth value
for (i in seq_along(bandwidth_values)) {
CV_scores[i] <- CV(bandwidth_values[i], data, kernel)
}
# Find the bandwidth value with the minimum cross-validation score
min_bandwidth <- bandwidth_values[which.min(CV_scores)]
return(min_bandwidth)
}
# Function to compute the optimal bandwidth using optimization
compute_CV_optim <- function(data, bandwidth_values, kernel) {
# Use optimization to find the bandwidth value that minimizes the cross-validation score
result <- optimize(CV, bandwidth_values, data = data, kernel = kernel)$minimum
return(result)
}
# Function to plot kernel density estimation for two datasets with different bandwidths
plot_kde_for_two_datasets <- function(data1, bandwidth1, data2, bandwidth2, kernel) {
# Define the range of values for evaluation
dataset1_name <- deparse(substitute(data1))
dataset2_name <- deparse(substitute(data2))
x1 <- seq(min(data1), max(data1), length.out = 100)
x2 <- seq(min(data2), max(data2), length.out = 100)
# Calculate density estimation for each dataset with respective bandwidth
density_estimation1 <- kde(x1, data1, bandwidth1, kernel)
density_estimation2 <- kde(x2, data2, bandwidth2, kernel)
# Create data frames for plotting
plot_data1 <- data.frame(x = x1, density = density_estimation1, dataset = rep("Dataset 1", length(x1)))
plot_data2 <- data.frame(x = x2, density = density_estimation2, dataset = rep("Dataset 2", length(x2)))
combined_plot_data <- rbind(plot_data1, plot_data2)
# Plot using ggplot2
ggplot(combined_plot_data, aes(x = x, y = density, color = dataset)) +
geom_line() +
scale_color_manual(values = c("red", "blue"), labels = c(dataset1_name, dataset2_name)) +
labs(x = "X", y = "Density", color = "Dataset") +
ggtitle(paste("Kernel Density Estimation for", dataset1_name, "and", dataset2_name))
}
###      PART    A      ###
# Read the data
data <- read.csv("StudentsPerformance.csv")
# Extract math scores
math_scores <- data$math.score
# Example usage:
bandwidth_seq <- c(0.09,0.5, 4.26, 6.8)  # Sequence of bandwidth values
plot_kde_for_bandwidth_seq(math_scores, bandwidth_seq, epanechnikov_kernel)
bandwidth <- 4.26
kernels <- list(gaussian_kernel = gaussian_kernel,
epanechnikov_kernel = epanechnikov_kernel,
uniform_kernel=uniform_kernel,
tridiagonal_kernel=tridiagonal_kernel)
plot_kde_for_kernel(math_scores, bandwidth, kernels)
# Extract the reading and writing scores from the data
reading_scores <- data$reading.score
writing_scores <- data$writing.score
# Define the range of bandwidth values to try
bandwidth_values <- c(0.09, 3.28, 4.26, 6.8, 20)
# Compute the optimal bandwidth using cross-validation for math scores
h_math <- compute_CV_optim(math_scores, bandwidth_values, gaussian_kernel)
# Compute the optimal bandwidth using cross-validation for reading scores
h_read <- compute_CV_optim(reading_scores, bandwidth_values, gaussian_kernel)
# Compute the optimal bandwidth using cross-validation for writing scores
h_write <- compute_CV_optim(writing_scores, bandwidth_values, gaussian_kernel)
# Load the required library for bandwidth selection
library(MASS)
# Compute bandwidth using biased cross-validation (bcv) for math scores
h_math_bcv <- bw.bcv(math_scores)
# Compute bandwidth using unbiased cross-validation (ucv) for math scores
h_math_ucv <- bw.ucv(math_scores)
# Compute bandwidth using biased cross-validation (bcv) for reading scores
h_read_bcv <- bw.bcv(reading_scores)
# Compute bandwidth using unbiased cross-validation (ucv) for reading scores
h_read_ucv <- bw.ucv(reading_scores)
# Compute bandwidth using biased cross-validation (bcv) for writing scores
h_write_bcv <- bw.bcv(writing_scores)
# Compute bandwidth using unbiased cross-validation (ucv) for writing scores
h_write_ucv <- bw.ucv(writing_scores)
# Create a summary table containing bandwidth values for each dataset and bandwidth selection method
summary_table <- data.frame(
Dataset = c("Math", "Reading", "Writing"),
CV = c(h_math, h_read, h_write),        # Bandwidth values computed using cross-validation
BCV = c(h_math_bcv, h_read_bcv, h_write_bcv),  # Bandwidth values computed using biased cross-validation
UCV = c(h_math_ucv, h_read_ucv, h_write_ucv)   # Bandwidth values computed using unbiased cross-validation
)
# Export the summary table to a CSV file named "summary_table.csv"
write.csv(summary_table, file = "summary_table.csv", row.names = FALSE)
View(data)
# Subset the data for completed cases and select only the desired columns
completed_cases <- subset(data, test.preparation.course == "completed" , select = c("math.score", "writing.score", "reading.score"))
# Subset the data for non-completed cases and select only the desired columns
non_completed_cases <- subset(data, test.preparation.course == "none", select = c("math.score", "reading.score", "writing.score"))
# Extract scores for completed and non-completed cases for each subject
math_completed <- completed_cases$math.score
math_non_completed <- non_completed_cases$math.score
reading_completed <- completed_cases$reading.score
reading_non_completed <- non_completed_cases$reading.score
writing_completed <- completed_cases$writing.score
writing_non_completed <- non_completed_cases$writing.score
# Create a list containing datasets for each subject
datasets <- list(math_completed, math_non_completed,
reading_completed, reading_non_completed,
writing_completed, writing_non_completed)
# Compute the optimal bandwidths using unbiased cross-validation (UCV) for each dataset
optimal_bandwidths <- lapply(datasets, compute_CV_optim, bandwidth_values = c(3.0, 6.0), kernel = gaussian_kernel)
View(optimal_bandwidths)
# Plot kernel density estimation for completed and non-completed cases for math scores
plot_kde_for_two_datasets(math_completed, 5.999924, math_non_completed, 5.353564, gaussian_kernel)
# Plot kernel density estimation for completed and non-completed cases for reading scores
plot_kde_for_two_datasets(reading_completed, 5.069659, reading_non_completed, 4.706739, gaussian_kernel)
# Plot kernel density estimation for completed and non-completed cases for writing scores
plot_kde_for_two_datasets(writing_completed, 3.851564, writing_non_completed, 4.8229, gaussian_kernel)
library(haven)  # For reading Stata files
library(dplyr)  # For data manipulation
library(ggplot2)  # For plotting
library(tidyr)  # For data reshaping
# Read the Stata file and create a data frame
df <- read_dta("childrenfinal.dta")
library(haven)  # For reading Stata files
library(dplyr)  # For data manipulation
library(ggplot2)  # For plotting
library(tidyr)  # For data reshaping
# Read the Stata file and create a data frame
df <- read_dta("childrenfinal.dta")
library(haven)  # For reading Stata files
library(dplyr)  # For data manipulation
library(ggplot2)  # For plotting
library(tidyr)  # For data reshaping
# Read the Stata file and create a data frame
df <- read_dta("childrenfinal.dta")
library(haven)  # For reading Stata files
library(dplyr)  # For data manipulation
library(ggplot2)  # For plotting
library(tidyr)  # For data reshaping
# Read the Stata file and create a data frame
df <- read_dta("childrenfinal.dta")
library(haven)  # For reading Stata files
library(dplyr)  # For data manipulation
library(ggplot2)  # For plotting
library(tidyr)  # For data reshaping
# Read the Stata file and create a data frame
df <- read_dta("childrenfinal.dta")
library(haven)  # For reading Stata files
library(dplyr)  # For data manipulation
library(ggplot2)  # For plotting
library(tidyr)  # For data reshaping
# Read the Stata file and create a data frame
df <- read_dta("childrenfinal.dta")
library(haven)  # For reading Stata files
library(dplyr)  # For data manipulation
library(ggplot2)  # For plotting
library(tidyr)  # For data reshaping
# Read the Stata file and create a data frame
df <- read_dta("childrenfinal.dta")
