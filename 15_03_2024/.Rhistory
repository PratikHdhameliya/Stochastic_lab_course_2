#results <- sapply(optimization_results, function(res) res$value)
# Plot the optimization results with correct x-axis labels
plot(1:length(output_of_marginal), output_of_marginal, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Marginal Density",
main = "Marginal Density for Optimized Parameter Sets", deparse(substitute(data)))
# Modify the y-axis labels to display the marginal density
axis(2, at = 1:length(output_of_marginal), labels = output_of_marginal)
}
Marginal_Density_and_plot(Optimized_data,data)
# Function to compute the marginal density and plot optimization results
Marginal_Density_and_plot <- function(optimization_results, data) {
# Define a function to compute the marginal density
Marginal_density <- function(params_data, data) {
p_data <- params_data[1]        # Probability parameter for data distribution
mean_1_data <- params_data[2]   # Mean parameter for the first data distribution
sd_1_data <- params_data[3]     # Standard deviation parameter for the first data distribution
mean_2_data <- params_data[4]   # Mean parameter for the second data distribution
sd_2_data <- params_data[5]     # Standard deviation parameter for the second data distribution
# Compute the marginal density for each data point
output <- sapply(data, function(x) {
marginal_dens_for_each_point <- log((p_data * dnorm(x, mean_1_data, sd_1_data)) +
((1 - p_data) * dnorm(x, mean_2_data, sd_2_data)))
return(marginal_dens_for_each_point)
})
return(sum(output))  # Sum of marginal densities
}
# Compute the marginal density for each optimization result
output_of_marginal <- numeric(length(optimization_results))
for (i in seq_along(optimization_results)) {
output_of_marginal[[i]] <- Marginal_density(params_data = optimization_results[[i]]$params, data = data)
}
# Extract the objective values from optimization results
#results <- sapply(optimization_results, function(res) res$value)
# Plot the optimization results with correct x-axis labels
plot(1:length(output_of_marginal), output_of_marginal, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Marginal Density",
main = "Marginal Density for Optimized Parameter Sets")
# Modify the y-axis labels to display the marginal density
axis(2, at = 1:length(output_of_marginal), labels = output_of_marginal)
}
Marginal_Density_and_plot(Optimized_data,data)
# Function to compute the marginal density and plot optimization results
Marginal_Density_and_plot <- function(optimization_results, data) {
# Define a function to compute the marginal density
Marginal_density <- function(params_data, data) {
p_data <- params_data[1]        # Probability parameter for data distribution
mean_1_data <- params_data[2]   # Mean parameter for the first data distribution
sd_1_data <- params_data[3]     # Standard deviation parameter for the first data distribution
mean_2_data <- params_data[4]   # Mean parameter for the second data distribution
sd_2_data <- params_data[5]     # Standard deviation parameter for the second data distribution
# Compute the marginal density for each data point
output <- sapply(data, function(x) {
marginal_dens_for_each_point <- log((p_data * dnorm(x, mean_1_data, sd_1_data)) +
((1 - p_data) * dnorm(x, mean_2_data, sd_2_data)))
return(marginal_dens_for_each_point)
})
return(sum(output))  # Sum of marginal densities
}
# Compute the marginal density for each optimization result
output_of_marginal <- numeric(length(optimization_results))
for (i in seq_along(optimization_results)) {
output_of_marginal[[i]] <- Marginal_density(params_data = optimization_results[[i]]$params, data = data)
}
# Extract the objective values from optimization results
#results <- sapply(optimization_results, function(res) res$value)
# Plot the optimization results with correct x-axis labels
plot(1:length(output_of_marginal), output_of_marginal, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Log-likelihood",
main = "Values of log-likelihood for Optimized Parameter Sets")
# Modify the y-axis labels to display the marginal density
axis(2, at = 1:length(output_of_marginal), labels = output_of_marginal)
}
Marginal_Density_and_plot(Optimized_data,data)
Marginal_Density_and_plot(optimization_results_quakes,quakes_data)
# Define a function to compute the final output based on given parameters
final_output <- function(params_data, data, params_UP) {
# Extract parameters for data and User Preference (UP)
p_data <- params_data[1]          # Probability parameter for data distribution
mean_1_data <- params_data[2]     # Mean parameter for the first data distribution
sd_1_data <- params_data[3]       # Standard deviation parameter for the first data distribution
mean_2_data <- params_data[4]     # Mean parameter for the second data distribution
sd_2_data <- params_data[5]       # Standard deviation parameter for the second data distribution
p_UP <- params_UP[1]              # Probability parameter for User Preference distribution
mean_1_UP <- params_UP[2]         # Mean parameter for the first User Preference distribution
sd_1_UP <- params_UP[3]           # Standard deviation parameter for the first User Preference distribution
mean_2_UP <- params_UP[4]         # Mean parameter for the second User Preference distribution
sd_2_UP <- params_UP[5]           # Standard deviation parameter for the second User Preference distribution
# Compute the final output based on the given data, parameters, and User Preference
output <- sapply(data, function(x) {
# Compute the numerator for the first distribution
numerator_1 <- log(p_data * dnorm(x, mean_1_data, sd_1_data)) *
((p_UP * dnorm(x, mean_1_UP, sd_1_UP)) /
(p_UP * dnorm(x, mean_1_UP, sd_1_UP) + (1 - p_UP) * dnorm(x, mean_2_UP, sd_2_UP)))
# Compute the numerator for the second distribution
numerator_2 <- log((1 - p_data) * dnorm(x, mean_2_data, sd_2_data)) *
(((1 - p_UP) * dnorm(x, mean_2_UP, sd_2_UP)) /
(p_UP * dnorm(x, mean_1_UP, sd_1_UP) + (1 - p_UP) * dnorm(x, mean_2_UP, sd_2_UP)))
# Return the sum of the two numerators
return(numerator_1 + numerator_2)
})
return(-(sum(output)))  # Minimize the negative sum of output
}
#############################################################################################################################################
#########Always keep in mind change upper bound after analyzing data otherwise this silly mistake will not produce proper output#############
#############################################################################################################################################
# Function to perform optimization using Differential Evolution (DE)
perform_optimization <- function(data, initial_params, num_iterations, stopping_criteria = c("iterations", "threshold"), threshold = 1e-3) {
# Initialize parameters for optimization
library(DEoptim)
params_UP <- initial_params
# Initialize list to store optimization results
optimization_results <- list()
# Perform optimization
for (i in 1:num_iterations) {
# Perform DE optimization
optimized_params <- DEoptim(final_output, data = data, params_UP = params_UP,
lower = c(0, 0, 0, 0, 0), upper = c(1, 1000, 1000, 1000, 1000),
control = list(trace = FALSE))
# Update parameters with the best parameters found
params_UP <- optimized_params$optim$bestmem
# Store optimization results
optimization_results[[i]] <- list(iteration = i, params = params_UP, value = optimized_params$optim$bestval)
# Print iteration number, best parameter values, and objective value
cat("Iteration:", i, "\n")
cat("Best parameters:", params_UP, "\n")
cat("Objective value:", optimized_params$optim$bestval, "\n\n")
# Check stopping criteria
if ("iterations" %in% stopping_criteria && i >= num_iterations) {
cat("Reached maximum number of iterations.\n")
break
}
if ("threshold" %in% stopping_criteria && i > 1) {
prev_params <- optimization_results[[i - 1]]$params
curr_params <- params_UP
dist <- sqrt(sum((curr_params - prev_params)^2))
if (dist < threshold) {
cat("Distance between consecutive parameter vectors below threshold.\n")
break
}
}
}
# Return optimization results
return(optimization_results)
}
## Be_ careful with optimization_results it should contain lot of optimized points where optimized parameters stored at
##params and params should have p value mean_1,sd_1,mean_2,sd_2 yeah that's all I guess :))
# Function to compute the marginal density and plot optimization results
Marginal_Density_and_plot <- function(optimization_results, data) {
# Define a function to compute the marginal density
Marginal_density <- function(params_data, data) {
p_data <- params_data[1]        # Probability parameter for data distribution
mean_1_data <- params_data[2]   # Mean parameter for the first data distribution
sd_1_data <- params_data[3]     # Standard deviation parameter for the first data distribution
mean_2_data <- params_data[4]   # Mean parameter for the second data distribution
sd_2_data <- params_data[5]     # Standard deviation parameter for the second data distribution
# Compute the marginal density for each data point
output <- sapply(data, function(x) {
marginal_dens_for_each_point <- log((p_data * dnorm(x, mean_1_data, sd_1_data)) +
((1 - p_data) * dnorm(x, mean_2_data, sd_2_data)))
return(marginal_dens_for_each_point)
})
return(sum(output))  # Sum of marginal densities
}
# Compute the marginal density for each optimization result
output_of_marginal <- numeric(length(optimization_results))
for (i in seq_along(optimization_results)) {
output_of_marginal[[i]] <- Marginal_density(params_data = optimization_results[[i]]$params, data = data)
}
# Extract the objective values from optimization results
#results <- sapply(optimization_results, function(res) res$value)
# Plot the optimization results with correct x-axis labels
plot(1:length(output_of_marginal), output_of_marginal, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Log-likelihood",
main = "Values of log-likelihood for Optimized Parameter Sets")
# Modify the y-axis labels to display the marginal density
axis(2, at = 1:length(output_of_marginal), labels = output_of_marginal)
}
# Load the readr package
library(readr)
# Read the data from the file
data <- read_csv("mixer_data.csv")
data<- data$x
hist(data)
m_1<-mean(data)
sd<-sd(data)
mean_1<-m_1-(sd/4)
mean_2<-m_1+(sd/4)
initial_params<-c(0.5,mean_1,sd,mean_2,sd)
Optimized_data<-perform_optimization(data,initial_params,num_iterations= 50, stopping_criteria = c("iterations", "threshold"), threshold = 1e-3)
# Load the glmnet package
install.packages("DEoptim")
library(glmnet)
library(DEoptim)
library(ggplot2)
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
beta_true <- c(1, 0, 2)
y <- X %*% beta_true + epsilon
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
result_CV<-run_adaptive_lasso_CV(beta_true)
run_DEoptim_BIC <- function(X, y, control_params, iterations = 1000) {
BIC_error <- function(lambda) {
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]
num_nonzero <- sum(coef_values != 0)
BIC_error_values <- RSS + log(length(y)) * num_nonzero
return(BIC_error_values)
}
lambda_min_values <- numeric(iterations)
BIC_error_values <- numeric(iterations)
for (i in 1:iterations) {
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control_params)
lambda_min_values[i] <- result$optim$bestmem
BIC_error_values[i] <- result$optim$bestval
}
results_df_BIC <- data.frame(lambda_min = lambda_min_values, BIC_error = BIC_error_values)
return(results_df_BIC)
}
Prediction_error<-function(lambda){
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
prediction_error <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
return(prediction_error)
}
#this function runs 1000 times and count error and lambda and give mean value
run_adaptive_lasso_CV <- function(beta_true) {
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09),iterations = 1000)
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09),iterations = 10)
Prediction_error<-function(lambda){
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
prediction_error <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
return(prediction_error)
}
View(results_BIC)
#this function runs 1000 times and count error and lambda and give mean value
run_adaptive_lasso_CV <- function(beta_true) {
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:5) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
result_CV<-run_adaptive_lasso_CV(beta_true)
lambda_CV<-avg(result_CV$lambda_min)
lambda_CV<-mean(result_CV$lambda_min)
Prediction_error_CV<-Prediction_error(lambda_CV)
View(result_CV)
lambda_BIC<-mean(results_BIC$lambda_min)
Prediction_error_BIC<-Prediction_error(lambda_BIC)
# Load necessary libraries
library(glmnet)
library(lars)
# Load the diabetes dataset from 'lars' package
data(diabetes)
# Split the dataset into predictors (X) and response (y)
X <- diabetes$x
y <- diabetes$y
# Convert predictors to a data frame (if necessary for other operations)
X_df <- as.data.frame(X)
# Fit an ordinary least squares model without an intercept
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Perform cross-validated adaptive Lasso to find the optimal lambda
adapt_lasso_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)))
lambda_min_adoptive <- adapt_lasso_cv$lambda.min
cat("Lambda for minimum CV error:", lambda_min_adoptive, "\n")
# Fit the adaptive Lasso model using the optimal lambda
adapt_lasso_fit <- glmnet(X, y, alpha = 1, lambda = lambda_min_adoptive, penalty.factor = as.vector(abs(ols_coef)))
# Extract coefficients and convert them to a dense vector
adapt_lasso_coef <- coef(adapt_lasso_fit)
adapt_lasso_coef_vector <- as.vector(adapt_lasso_coef)
# Separate weights and intercept from the coefficient vector
weight <- adapt_lasso_coef_vector[-1]  # Exclude the first element (intercept)
intercept <- adapt_lasso_coef_vector[1]  # The first element is the intercept
# Calculate prediction error manually
prediction_error <- as.numeric(t(y - ((X %*% weight) + intercept)) %*% (y - ((X %*% weight) + intercept)))
# Compute residual sum of squares (RSS)
RSS <- sum((y - predict(adapt_lasso_fit, newx = X, s = lambda_min_adoptive))^2)
# Compute the number of nonzero coefficients
num_nonzero <- sum(adapt_lasso_coef != 0)
# Calculate BIC error using the formula
n <- nrow(X)
BIC_error_value_dia <- RSS + log(n) * num_nonzero
# Print results
cat("Prediction error:", prediction_error)
cat("BIC error value:", BIC_error_value_dia)
cat("Cross-validation error value:", min(adapt_lasso_cv$cvm))
# Plot the cross-validation results
plot(adapt_lasso_cv)
# Create a data frame
results_df <- data.frame(
Prediction_error_diabetes = prediction_error,
BIC_error_diabetes = BIC_error_value_dia,
CV_error_diabetes = min(adapt_lasso_cv$cvm)
)
# Export the data frame to a CSV file
write.csv(results_df, "results.csv", row.names = FALSE)
# Plot the cross-validation results
plot(adapt_lasso_cv,ylab = "Mean-Squared Error for Diabetes data set")
# Create a data frame
results_df <- data.frame(
Prediction_error_diabetes = prediction_error,
BIC_error_diabetes = BIC_error_value_dia,
CV_error_diabetes = min(adapt_lasso_cv$cvm)
)
# Export the data frame to a CSV file
write.csv(results_df, "results_of_diabetes.csv", row.names = FALSE)
#Loading the data
library(haven)
library(dplyr)
library(ggplot2)
library(tidyr)
df<-read_dta("childrenfinal.dta")
df<-read_dta("childrenfinal")
# Define a function to compute the final output based on given parameters
final_output <- function(params_data, data, params_UP) {
# Extract parameters for data and User Preference (UP)
p_data <- params_data[1]          # Probability parameter for data distribution
mean_1_data <- params_data[2]     # Mean parameter for the first data distribution
sd_1_data <- params_data[3]       # Standard deviation parameter for the first data distribution
mean_2_data <- params_data[4]     # Mean parameter for the second data distribution
sd_2_data <- params_data[5]       # Standard deviation parameter for the second data distribution
p_UP <- params_UP[1]              # Probability parameter for User Preference distribution
mean_1_UP <- params_UP[2]         # Mean parameter for the first User Preference distribution
sd_1_UP <- params_UP[3]           # Standard deviation parameter for the first User Preference distribution
mean_2_UP <- params_UP[4]         # Mean parameter for the second User Preference distribution
sd_2_UP <- params_UP[5]           # Standard deviation parameter for the second User Preference distribution
# Compute the final output based on the given data, parameters, and User Preference
output <- sapply(data, function(x) {
# Compute the numerator for the first distribution
numerator_1 <- log(p_data * dnorm(x, mean_1_data, sd_1_data)) *
((p_UP * dnorm(x, mean_1_UP, sd_1_UP)) /
(p_UP * dnorm(x, mean_1_UP, sd_1_UP) + (1 - p_UP) * dnorm(x, mean_2_UP, sd_2_UP)))
# Compute the numerator for the second distribution
numerator_2 <- log((1 - p_data) * dnorm(x, mean_2_data, sd_2_data)) *
(((1 - p_UP) * dnorm(x, mean_2_UP, sd_2_UP)) /
(p_UP * dnorm(x, mean_1_UP, sd_1_UP) + (1 - p_UP) * dnorm(x, mean_2_UP, sd_2_UP)))
# Return the sum of the two numerators
return(numerator_1 + numerator_2)
})
return(-(sum(output)))  # Minimize the negative sum of output
}
#############################################################################################################################################
#########Always keep in mind change upper bound after analyzing data otherwise this silly mistake will not produce proper output#############
#############################################################################################################################################
# Function to perform optimization using Differential Evolution (DE)
perform_optimization <- function(data, initial_params, num_iterations, stopping_criteria = c("iterations", "threshold"), threshold = 1e-3) {
# Initialize parameters for optimization
library(DEoptim)
params_UP <- initial_params
# Initialize list to store optimization results
optimization_results <- list()
# Perform optimization
for (i in 1:num_iterations) {
# Perform DE optimization
optimized_params <- DEoptim(final_output, data = data, params_UP = params_UP,
lower = c(0, 0, 0, 0, 0), upper = c(1, 1000, 1000, 1000, 1000),
control = list(trace = FALSE))
# Update parameters with the best parameters found
params_UP <- optimized_params$optim$bestmem
# Store optimization results
optimization_results[[i]] <- list(iteration = i, params = params_UP, value = optimized_params$optim$bestval)
# Print iteration number, best parameter values, and objective value
cat("Iteration:", i, "\n")
cat("Best parameters:", params_UP, "\n")
cat("Objective value:", optimized_params$optim$bestval, "\n\n")
# Check stopping criteria
if ("iterations" %in% stopping_criteria && i >= num_iterations) {
cat("Reached maximum number of iterations.\n")
break
}
if ("threshold" %in% stopping_criteria && i > 1) {
prev_params <- optimization_results[[i - 1]]$params
curr_params <- params_UP
dist <- sqrt(sum((curr_params - prev_params)^2))
if (dist < threshold) {
cat("Distance between consecutive parameter vectors below threshold.\n")
break
}
}
}
# Return optimization results
return(optimization_results)
}
## Be_ careful with optimization_results it should contain lot of optimized points where optimized parameters stored at
##params and params should have p value mean_1,sd_1,mean_2,sd_2 yeah that's all I guess :))
# Function to compute the marginal density and plot optimization results
Marginal_Density_and_plot <- function(optimization_results, data) {
# Define a function to compute the marginal density
Marginal_density <- function(params_data, data) {
p_data <- params_data[1]        # Probability parameter for data distribution
mean_1_data <- params_data[2]   # Mean parameter for the first data distribution
sd_1_data <- params_data[3]     # Standard deviation parameter for the first data distribution
mean_2_data <- params_data[4]   # Mean parameter for the second data distribution
sd_2_data <- params_data[5]     # Standard deviation parameter for the second data distribution
# Compute the marginal density for each data point
output <- sapply(data, function(x) {
marginal_dens_for_each_point <- log((p_data * dnorm(x, mean_1_data, sd_1_data)) +
((1 - p_data) * dnorm(x, mean_2_data, sd_2_data)))
return(marginal_dens_for_each_point)
})
return(sum(output))  # Sum of marginal densities
}
# Compute the marginal density for each optimization result
output_of_marginal <- numeric(length(optimization_results))
for (i in seq_along(optimization_results)) {
output_of_marginal[[i]] <- Marginal_density(params_data = optimization_results[[i]]$params, data = data)
}
# Extract the objective values from optimization results
#results <- sapply(optimization_results, function(res) res$value)
# Plot the optimization results with correct x-axis labels
plot(1:length(output_of_marginal), output_of_marginal, type = "l", col = "blue", lwd = 2,
xlab = "Iteration", ylab = "Log-likelihood",
main = "Values of log-likelihood for Optimized Parameter Sets")
# Modify the y-axis labels to display the marginal density
axis(2, at = 1:length(output_of_marginal), labels = output_of_marginal)
}
# Load the readr package
library(readr)
# Read the data from the file
data <- read_csv("mixer_data.csv")
data <- data$x
# Plot a histogram of the data
hist(data)
# Compute the mean and standard deviation of the data
m_1 <- mean(data)
sd <- sd(data)
# Define initial parameter values for optimization
mean_1 <- m_1 - (sd / 4)
mean_2 <- m_1 + (sd / 4)
initial_params <- c(0.5, mean_1, sd, mean_2, sd)
# Perform optimization to find the best parameters
Optimized_data <- perform_optimization(data, initial_params, num_iterations = 100,
stopping_criteria = c("iterations", "threshold"),
threshold = 1e-3)
##########PRAT(a)#####################################################################
# Load the required libraries
library(haven)
library(dplyr)
# Read the Stata dataset "childrenfinal.dta" into R
data <- read_dta("childrenfinal.dta")
