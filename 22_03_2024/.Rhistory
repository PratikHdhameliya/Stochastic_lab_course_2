# Compute residual sum of squares (RSS)
RSS <- sum((y - predict(adapt_lasso_fit, newx = X, s = lambda_min_adoptive))^2)
# Compute the number of nonzero coefficients
num_nonzero <- sum(adapt_lasso_coef != 0)
# Calculate BIC error using the formula
n <- nrow(X)
BIC_error_value_dia <- RSS + log(n) * num_nonzero
# Print results
print("Prediction error:", prediction_error)
# Load necessary libraries
library(glmnet)
library(lars)
# Load the diabetes dataset from 'lars' package
data(diabetes)
# Split the dataset into predictors (X) and response (y)
X <- diabetes$x
y <- diabetes$y
# Convert predictors to a data frame (if necessary for other operations)
X_df <- as.data.frame(X)
# Fit an ordinary least squares model without an intercept
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Perform cross-validated adaptive Lasso to find the optimal lambda
adapt_lasso_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)))
lambda_min_adoptive <- adapt_lasso_cv$lambda.min
cat("Lambda for minimum CV error:", lambda_min_adoptive, "\n")
# Fit the adaptive Lasso model using the optimal lambda
adapt_lasso_fit <- glmnet(X, y, alpha = 1, lambda = lambda_min_adoptive, penalty.factor = as.vector(abs(ols_coef)))
# Extract coefficients and convert them to a dense vector
adapt_lasso_coef <- coef(adapt_lasso_fit)
adapt_lasso_coef_vector <- as.vector(adapt_lasso_coef)
# Separate weights and intercept from the coefficient vector
weight <- adapt_lasso_coef_vector[-1]  # Exclude the first element (intercept)
intercept <- adapt_lasso_coef_vector[1]  # The first element is the intercept
# Calculate prediction error manually
prediction_error <- as.numeric(t(y - ((X %*% weight) + intercept)) %*% (y - ((X %*% weight) + intercept)))
# Compute residual sum of squares (RSS)
RSS <- sum((y - predict(adapt_lasso_fit, newx = X, s = lambda_min_adoptive))^2)
# Compute the number of nonzero coefficients
num_nonzero <- sum(adapt_lasso_coef != 0)
# Calculate BIC error using the formula
n <- nrow(X)
BIC_error_value_dia <- RSS + log(n) * num_nonzero
# Print results
cat("Prediction error:", prediction_error)
cat("BIC error value:", BIC_error_value_dia)
cat("Cross-validation error value:", min(adapt_lasso_cv$cvm))
# Plot the cross-validation results
plot(adapt_lasso_cv)
# Load the glmnet package
library(glmnet)
install.packages("DEoptim")
library(DEoptim)
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
beta_true <- c(3, 0, 1/n)
y <- X %*% beta_true + epsilon
#this function runs 1000 times and count error and lambda and give mean value
run_adaptive_lasso_CV <- function(beta_true) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
result_CV<-run_adaptive_lasso_CV(beta_true)
run_DEoptim_BIC <- function(X, y, control_params, iterations = 1000) {
BIC_error <- function(lambda) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]
num_nonzero <- sum(coef_values != 0)
BIC_error_values <- RSS + log(length(y)) * num_nonzero
return(BIC_error_values)
}
lambda_min_values <- numeric(iterations)
BIC_error_values <- numeric(iterations)
for (i in 1:iterations) {
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control_params)
lambda_min_values[i] <- result$optim$bestmem
BIC_error_values[i] <- result$optim$bestval
}
results_df_BIC <- data.frame(lambda_min = lambda_min_values, BIC_error = BIC_error_values)
return(results_df_BIC)
}
# Example usage:
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09))
# Load the glmnet package
install.packages("DEoptim")
library(glmnet)
library(DEoptim)
set.seed(123)
n <- 100
X <- cbind(rep(1, n), seq(1, n), rep(c(1, 0), times = n/2))
epsilon <- rnorm(n)
beta_true <- c(3, 0, 1/n)
y <- X %*% beta_true + epsilon
#this function runs 1000 times and count error and lambda and give mean value
run_adaptive_lasso_CV <- function(beta_true) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(1000)
cv_error_values <- numeric(1000)
BIC_error_values <- numeric(1000)
prediction_error <- numeric(1000)
# Loop over i from 1 to 1000
for (i in 1:1000) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
result_CV<-run_adaptive_lasso_CV(beta_true)
ggplot(result_CV, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
library(ggplot2)
ggplot(result_CV, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error")
run_DEoptim_BIC <- function(X, y, control_params, iterations = 1000) {
BIC_error <- function(lambda) {
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]
num_nonzero <- sum(coef_values != 0)
BIC_error_values <- RSS + log(length(y)) * num_nonzero
return(BIC_error_values)
}
lambda_min_values <- numeric(iterations)
BIC_error_values <- numeric(iterations)
for (i in 1:iterations) {
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control_params)
lambda_min_values[i] <- result$optim$bestmem
BIC_error_values[i] <- result$optim$bestval
}
results_df_BIC <- data.frame(lambda_min = lambda_min_values, BIC_error = BIC_error_values)
return(results_df_BIC)
}
# Example usage:
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09))
# Example usage:
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09),iterations = 10)
ggplot(result_BIC, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "BIC Error", title = "Lambda vs BIC Error")
ggplot(results_BIC, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "BIC Error", title = "Lambda vs BIC Error")
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09),iterations = 10)
ggplot(result_BIC, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "BIC Error", title = "Lambda vs BIC Error for another Beta")
ggplot(results_BIC, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "BIC Error", title = "Lambda vs BIC Error for another Beta")
beta_true <- c(3, 0, 5)
y <- X %*% beta_true + epsilon
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09),iterations = 10)
ggplot(results_BIC, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "BIC Error", title = "Lambda vs BIC Error for another Beta")
View(results_BIC)
# Load necessary libraries
library(glmnet)
library(lars)
# Load the diabetes dataset from 'lars' package
data(diabetes)
# Split the dataset into predictors (X) and response (y)
X <- diabetes$x
y <- diabetes$y
# Convert predictors to a data frame (if necessary for other operations)
X_df <- as.data.frame(X)
# Fit an ordinary least squares model without an intercept
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Perform cross-validated adaptive Lasso to find the optimal lambda
adapt_lasso_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)))
lambda_min_adoptive <- adapt_lasso_cv$lambda.min
# Load necessary libraries
library(glmnet)
library(lars)
# Load the diabetes dataset from 'lars' package
data(diabetes)
# Split the dataset into predictors (X) and response (y)
X <- diabetes$x
y <- diabetes$y
# Convert predictors to a data frame (if necessary for other operations)
X_df <- as.data.frame(X)
# Fit an ordinary least squares model without an intercept
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
# Perform cross-validated adaptive Lasso to find the optimal lambda
adapt_lasso_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)))
lambda_min_adoptive <- adapt_lasso_cv$lambda.min
View(diabetes)
# Load necessary libraries
library(glmnet)
library(lars)
# Load the diabetes dataset from 'lars' package
data(diabetes)
# Split the dataset into predictors (X) and response (y)
X <- diabetes$x
y <- diabetes$y
# Convert predictors to a data frame (if necessary for other operations)
X_df <- as.data.frame(X)
# Fit an ordinary least squares model without an intercept
ols_fit <- lm(y ~ X + 0)
ols_coef <- coef(ols_fit)
run_adaptive_lasso_CV <- function(beta_true) {
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(40)
cv_error_values <- numeric(40)
BIC_error_values <- numeric(40)
prediction_error <- numeric(40)
# Loop over i from 1 to 1000
for (i in 1:40) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
result_CV<-run_adaptive_lasso_CV(beta_true)
run_adaptive_lasso_CV <- function(beta_true) {
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(40)
cv_error_values <- numeric(40)
BIC_error_values <- numeric(40)
prediction_error <- numeric(40)
# Loop over i from 1 to 1000
for (i in 1:40) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
return(results)
}
result_CV<-run_adaptive_lasso_CV(beta_true)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(40)
cv_error_values <- numeric(40)
BIC_error_values <- numeric(40)
prediction_error <- numeric(40)
# Loop over i from 1 to 1000
for (i in 1:40) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
n<-nrow(X)
# Initialize storage vectors
lambda_min_adoptive_values <- numeric(40)
cv_error_values <- numeric(40)
BIC_error_values <- numeric(40)
prediction_error <- numeric(40)
# Loop over i from 1 to 1000
for (i in 1:40) {
# Perform cross-validated Lasso
adapt_model_cv <- cv.glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Store lambda and CV error
lambda_min_adoptive_values[i] <- adapt_model_cv$lambda.min
cv_error_values[i] <- min(adapt_model_cv$cvm)
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
# Extract coefficients and compute statistics
coef_values <- coef(adapt_model_fit, s = adapt_model_cv$lambda.min)[-1]  # Exclude intercept
prediction_error[i] <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
# Compute residual sum of squares
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = adapt_model_cv$lambda.min))^2)
# Compute number of nonzero coefficients
num_nonzero <- sum(coef_values != 0)
n<-nrow(X)
# Compute and store BIC error
BIC_error_values[i] <- RSS + log(n) * num_nonzero
}
# Calculate and return mean results
results <- data.frame(
lambda_min =       lambda_min_adoptive_values,
cv_error =         cv_error_values,
BIC_error =        BIC_error_values,
prediction_error = prediction_error
)
View(results)
lambda_CV<-mean(result_CV$lambda_min)
lambda_CV<-mean(results$lambda_min)
ggplot(results, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for Diabetes data")
library(ggplot2)
lambda_CV<-mean(results$lambda_min)
ggplot(results, aes(x = lambda_min, y = cv_error)) +
geom_line() +
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for Diabetes data")
run_DEoptim_BIC <- function(X, y, control_params, iterations = 40) {
BIC_error <- function(lambda) {
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]
num_nonzero <- sum(coef_values != 0)
BIC_error_values <- RSS + log(length(y)) * num_nonzero
return(BIC_error_values)
}
lambda_min_values <- numeric(iterations)
BIC_error_values <- numeric(iterations)
for (i in 1:iterations) {
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control_params)
lambda_min_values[i] <- result$optim$bestmem
BIC_error_values[i] <- result$optim$bestval
}
results_df_BIC <- data.frame(lambda_min = lambda_min_values, BIC_error = BIC_error_values)
return(results_df_BIC)
}
# Example usage:
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09),iterations = 40)
library(DEoptim)
run_DEoptim_BIC <- function(X, y, control_params, iterations = 40) {
BIC_error <- function(lambda) {
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
RSS <- sum((y - predict(adapt_model_fit, newx = X, s = lambda))^2)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]
num_nonzero <- sum(coef_values != 0)
BIC_error_values <- RSS + log(length(y)) * num_nonzero
return(BIC_error_values)
}
lambda_min_values <- numeric(iterations)
BIC_error_values <- numeric(iterations)
for (i in 1:iterations) {
result <- DEoptim(BIC_error, lower = 0, upper = 100, control = control_params)
lambda_min_values[i] <- result$optim$bestmem
BIC_error_values[i] <- result$optim$bestval
}
results_df_BIC <- data.frame(lambda_min = lambda_min_values, BIC_error = BIC_error_values)
return(results_df_BIC)
}
results_BIC <- run_DEoptim_BIC(X, y, control_params = DEoptim.control(itermax = 100, F = 0.05, CR = 0.09),iterations = 40)
lambda_BIC<-mean(results_BIC$lambda_min)
ggplot(results_BIC, aes(x = lambda_min, y = BIC_error)) +
geom_line() +
labs(x = "Lambda", y = "BIC Error", title = "Lambda vs BIC Error for Diabetes data")
Prediction_error<-function(lambda){
# Fit model using determined lambda_min
adapt_model_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE, standardize = TRUE)
coef_values <- coef(adapt_model_fit, s = lambda)[-1]  # Exclude intercept
prediction_error <- t(y - X %*% coef_values) %*% (y - X %*% coef_values)
return(prediction_error)
}
Prediction_error_BIC<-Prediction_error(lambda_BIC)
Prediction_error_CV<-Prediction_error(lambda_CV)
# Create a data frame
results_df <- data.frame(
lambda_CV = lambda_CV,
lambda_BIC = lambda_BIC,
Prediction_error_BIC = Prediction_error_BIC,
Prediction_error_CV = Prediction_error_CV,
)
ggplot(results, aes(x = lambda_min, y = cv_error)) +
geom_smooth(method = "loess", se = FALSE) +  # Use loess smoothing method
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for Diabetes data")
results_df <- data.frame(
lambda_CV = lambda_CV,
lambda_BIC = lambda_BIC,
Prediction_error_BIC = Prediction_error_BIC,
Prediction_error_CV = Prediction_error_CV
)
View(results_df)
results_df <- data.frame(
lambda_CV = lambda_CV,
lambda_BIC = lambda_BIC,
Prediction_error_BIC = Prediction_error_BIC,
Prediction_error_CV = Prediction_error_CV
)
# Export the data frame to a CSV file
write.csv(results_df, "results_diabetes.csv", row.names = FALSE)
data<-read.csv(resultsofcvnewbeta)
data<-read.csv("resultsofcvnewbeta.csv")
ggplot(data, aes(x = lambda_min, y = cv_error)) +
geom_smooth(method = "loess", se = FALSE) +  # Use loess smoothing method
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for another beta")
lasso_fit <- glmnet(X, y, alpha = 1)
plot(lasso_fit)
adapt_lasso_fit <- glmnet(X, y, alpha = 1, penalty.factor = as.vector(abs(ols_coef)), intercept = FALSE)
plot(adapt_lasso_fit)
plot(adapt_lasso_fit,xvar = "lmabda",label = TRUE)
plot(adapt_lasso_fit,xvar = "lambda",label = TRUE)
plot(lasso_fit,xvar = "lambda",label = TRUE)
ggplot(data, aes(x = lambda_min, y = cv_error)) +
geom_smooth(method = "loess", se = FALSE) +  # Use loess smoothing method
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for another beta")
ggplot(data, aes(x = lambda_min, y = cv_error)) +
geom_smooth(method = "loess", se = FALSE, span = 0.5) +  # Adjust the span parameter
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for another beta")
ggplot(data, aes(x = lambda_min, y = cv_error)) +
geom_smooth(method = "loess", se = FALSE, span = 0.7) +  # Adjust the span parameter
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for another beta")
ggplot(data, aes(x = lambda_min, y = cv_error)) +
geom_smooth(method = "loess", se = FALSE, span = 1) +  # Adjust the span parameter
labs(x = "Lambda", y = "CV Error", title = "Lambda vs CV Error for another beta")
gc()
gc()
gc()
gc()
